\chapter{Simulations and reconstructions}
\label{ch:reconstructions}


In order to develop and test reconstructions simulations are used. The in- and output are compared to test if the expected accuracy is achieved. Simulations are also used to better understand measured data, for example to test the effects of the Poisson distributed particle sampling and the detector response. First the simulations of the air showers which are the input for the detector simulation are described in the following section. Both Monte Carlo simulations of air showers and parametric approximations describing the time and density structures of the shower fronts are used.


\section{Monte Carlo extensive air shower simulation}

For the simulation of cosmic-ray induced air showers there are several programs available, most notably \corsika and \aires \cite{sciutto1999aires}. \corsika was chosen because development is still active, unlike \aires which received its last update in 2006. \corsika also includes updated models based on recent \lhc data. Additionally, \corsika is widely used by other cosmic-ray experiments.

\corsika version 74000 is used for all simulations reported in this thesis. \corsika provides the choice from many models for various interactions. For high energy hadronic interactions; DPMJET, EPOS \lhc \cite{pierog2013epos}, NEXUS, QGSJET, QGSJETII-04 \cite{ostapchenko2013qgsjetii}, SIBYLL, and VENUS are available. For hadrons with energies below \SI{80}{\GeV} GHEISHA \cite{fesefeldt1985}, FLUKA, or URQMD can be selected. Electromagnetic interactions can be treated by EGS4 \cite{egs4} code or by NKG formulas.

From these options QGSJETII, GHEISHA and EGS4 were chosen. QGSJETII was chosen because the model has been tuned with \lhc data, and performs similar to EPOS, though both are not perfect yet (see ICRC2015 talks). Moreover, QGSJETII is easier to setup and faster to run [ref manual] than EPOS. All other options were left to their default values. The thinning algorithms were always disabled. For this configuration of \corsika an executable was compiled.  All simulations were performed using that same executable.

% From CORSIKA manual regarding model speed:
% - The QGSJETII04 option needs about 3 times more CPU-time than the HDPM  option (NKG enabled, EGS4 disabled).
% - The EPOS option needs roughly 7.5 times more CPU-time than the VENUS option (E0 = 1015 eV, NKG enabled, EGS4 disabled).
% - The VENUS option needs roughly 15 times more CPU-time than the HDPM option (NKG enabled, EGS4 disabled).
% So the EPOS option needs 7.5 x 15 = 112.5 times more CPU-time than the QGSJETII option. However, a large portion of simulation is not spent in the hadron models.


\subsection{Shower parameters}

The simulations are steered by an input file, most important parameters can be set here. The default options are used whenever applicable and believing in sensibly chosen defaults. The magnetic field and observation level values have been modified to values suitable for Amsterdam [ref]. The observation level for all simulations is set to\SI{10}{\meter} above sea level, which is relevant for the Science Park, where ground level is \SI{3.7}{\meter} below sea level, and the detector stations are on the roofs of buildings. The seeds for the random number generators (first for the hadron shower, second for EGS4) are randomly chosen and required to be unique for each simulation. The two seed values can be used as identifiers for specific simulations. Each combination of two seeds is unique in the dataset of simulations. A single air shower is simulated per run.

Each run is for a specific particle with a specific energy between \SIrange{e11}{e18}{\eV} in steps of $\log \left(E / \si{\eV}\right) = 0.5$. The zenith angle is also set for each run and is varied from\SIrange{0}{60}{\degree} in steps of \SI{7.5}{\degree}. The azimuthal angle is usually set to \SI{0}{\degree} (\hisparc coordinate system), but can be varied in steps of \SI{45}{\degree}.


\subsubsection{Primary particle}

Given the high abundance of protons as primary cosmic ray particles and iron with a relative high abundance for heavier nuclei these are the particles that were most focused on for the simulations. However, small datasets with other primaries have also been created. For protons all possible combinations of energy and zenith have been simulated at least 10 times. In \cref{fig:simulations_proton_energy_zenith} the number of simulated showers for each combination of the energy and zenith parameters for proton primaries can be seen. The focus was on the showers most often seen by \hisparc, i.e. primary energy of \SI{e15}{\eV} at \SI{22.5}{\degree} zenith.


\subsubsection{Energy cuts}

The \corsika simulations will be used as input for detector simulations. Therefore, if in the simulation particles are created which will not be detected by a \hisparc detector the particle can be ignored. In \corsika energy cuts can be set for different types of particles, as soon as particle of that type drop below that energy value they are no longer tracked in the simulation. The energy cuts have been set to \SI{300}{\MeV} for hadrons and muons, and \SI{3}{\MeV} for electrons and photons [ref LiO Tom]. Below these energies the particles will no longer produce significant signals in the detector and are quickly stopped/slowed by the atmosphere and decay [ref PDG, fig 28.5].


\subsection{Stoomboot}

To run a significant number of simulations the local \nikhef computer cluster 'Stoomboot' was utilized. This cluster used to have around 300 CPUs available, but was expanded in February 2014 to over 800 CPUs. Job queueing uses a fair-use policy to give each group at Nikhef equal opportunity to use computation time.

Simulation time for each simulation is different because the total number of particles in a shower will be different each run, even with the same starting paramters but with different seeds for the random number generators. Stoomboot has a maximum job time of \SI{96}{\hour}. This allows for showers with primary energies up to \SI{e17}{\eV}, which take around 60 hours to complete. Showers with primary energies less than \SI{e14}{\eV} are very quick and take only a minute [check] to complete. A large sample of showers can easily be generated by running many jobs simultaneously.

Cosmic rays with energies above \SI{e17}{\eV} have been observed, so some simulations at higher energies are desired. During the 2014 Christmas holidays 285 jobs at energies \SIlist{e17.5;e18}{\eV} were run. To allow this the maximum job time had been manually extended by a Stoomboot administrator. The longest jobs took over \SI{480}{\hour} (i.e.\SI{20}{\day}) to complete. In total over [check] \SI{50}{\year} of CPU time was used to generate the entire simulation sample.

\begin{figure}
    \centering
    \input{plots/reconstructions/shower_walltime}
    \caption{\captitle{Walltime for \corsika simulations.} Vertial lines indicate the walltime limits of the different Stoomboot queues.}
    \label{fig:simulations_shower_walltime}
\end{figure}


\subsection{No thinning}

% Explain how thinning helps to reduce computation time
% - Keep one particle if energy drops below fraction, only up to given weight
% - To reduce storage thin particles close to core at observation level
\corsika provides a thinning mechanism to reduce the computation time and storage requirements. This could allow for the simulation of even more energetic air showers, for cases where otherwise the simulation time would become unreasonable. Thinning works by taking particles emerging from interactions which are below a given fraction of the primary particle energy and drops all but one of those. The particle that is kept is given a weight to represent the number of dropped particles. These weights are then included with the particle information for the particles at the observation level. Additionally, particles close to the shower core at the observation level, where the detectors are likely saturated, may be thinned further to reduce the required storage size. The thinned shower output format is currently incompatible with the \hisparc detector simulation. The detector simulations requires individual particles, otherwise the particle density needs to be reconstructed to apply Poisson statistics to achieve the number of particles in a detector. A dethinning algorithm exists \cite{stokes2012} which can unravel the weighted particles into separate particles, of course some information is already lost.

% We do not use thinning because Stoomboot is fast enough for some 1e18 eV
The thinning mechanism has not been enabled because with Stoomboot enough computational time was available to run showers with energies up to \SI{e18}{\eV} without thinning. As shown by [LiO Sabine] the number of detected air showers by the Science Park cluster with primary energies above \SI{e18}{\eV} is low. Therefore simulated showers with higher energies are not immediately necessary, moreover, the extremely large file sizes make it harder to use them.


\subsection{Simulations catalogue}

Using Stoomboot a collection of over \num{70000} simulated showers has been created. This data, approximately \SI{20}{\tera\byte} [check], is stored on the \hisparc data server (trave) which has \SI{37}{\tera\byte} space. This space is shared with the raw \hisparc data (cosmic and weather), \knmi lightning data, and analysis files from various \hisparc collaborators.

For easy use with the \sapphire framework the \corsika simulations are converted from the sequential unformatted \fortran data format to \hdf format. This is done using a modified version of the the Python \corsika Reader package \cite{gonzalez2011}. During the conversion the values are (when applicable) converted to the units and coordinate systems used in \hisparc [see appendix].

After the initial conversion further optimisation is performed by sorting and indexing the 'x' coordinate column for the particles. In the detector simulation particles are queried from the \corsika data file using x and y coordinates. With the index it is very easy for the software to know which part of the data it needs. Since the data is sorted (on disk) only a small part of the data needs to be read into memory. If the data was not sorted more chunks of the file would need to be read into memory. In some cases the sorting speeds up simulations by a factor of 50 \cite{pytables:optimization}.

[TODO: Update plot with intermediate energies, and 'color bar', also one for iron?]

\begin{figure}
    \centering
    \input{plots/reconstructions/energy_zenith_proton}
    \caption{\captitle{Proton simulations overview.} The square of the area of the rectangles indicate the number simulations with that energy and zenithb combinations with a proton primary.}
    \label{fig:simulations_proton_energy_zenith}
\end{figure}


\subsubsection{Simulation overview}

Whenever new simulations are performed they are automatically converted to \hdf and added to an overview. The overview can for instance be used to find a suitable \corsika simulation for a detector simulation. The overview contains the variable input parameters, the number of particles on ground level per particle type, and the height of first interaction for each simulation. From this data we can compare the shower sizes for proton primary showers as a function of energy and zenith angle. The shower sizes, in this case the sum of all leptons, are shown in \cref{fig:simulations_shower_sizes}. There is a clear separation between the shower sizes for particle of different energies, in steps of $\log \left(E / {\si{\eV}}\right) = 0.5$, at the same inclination. There may be some overlap between showers of adjacent energies, but only at the few percentile level.

\begin{figure}
    \centering
    \input{plots/reconstructions/shower_sizes}
    \caption{\captitle{Shower size versus zenith angle and primary energy.}             Each line indicates the median shower size for a specific energy as a function of zenith angle. The gray area indicates the 16th and 84th percentile.}
    \label{fig:simulations_shower_sizes}
\end{figure}


\section{Detector simulation}

The detector and station responses and accuracies reported in [chaptersdetector/station] are implemented in the detector simulations. These routines provide the station response to air showers.

As input the \corsika showers or parameterized descriptions of the shower front can be used. The same cluster objects, containing the station and detector positions, which are used in reconstructions are used for simulations. These objects can contain stations and detectors based on real configurations or a fictional station arangement.


\subsection{Simulation steps}

[todo]
The detector simulation requires a cluster object, choice of shower input, and a number of iterations. Depending on the type of shower input some other options may be available. Finally a sensible maximum core distance and number of iterations can be chosen. For each iteration the following steps are taken.

Generate shower parameters; core position, azimuth angle, and timestamp. A random position within a given radius around the cluster origin is generated for the shower core position.

Instead of shifting and rotating the particles in the \corsika file thecluster and detector positions are shifted. Performing computations onthe \corsika data prevents queries from using the table indexes.

Then for each station each detector is simulated.

For each detector the particles which pass through it are selected, then the detector response is determined (i.e. total signal strength and time of first particle). Though the orientation of the individual detectors on Science Park is known, this information is not used in the default simulation. Instead a square with \SI{.5}{\meter\square} area is used for a simpler query which simply checks for all leptons (and gamma?) where the x and y are within .. of he detector center. If the oriëntation of detectors is alsoused the boundary lines have to be computed and a more complicated queryis used. This query can still used the data index by first selecting alarger bounding square in which the whole detector is contained regardles of oriëntation. This approximation means that at most\SI{29.2}{\percent} of the detector surface is not in its actual location, but still connected to the detector. However, other source for error: Poisson, GPS, detector measurement.

Then it is determined if the station triggered or not. This uses the same trigger logic as the \hisparc stations. For 4-detector stationseither 2-high or 3-low, and for 2-detector stations 2-low. The thresholds are set at a fraction of a particle, MPV peak.. Relation between pulse height and integral in data, spread..

If a station triggered its event is stored, containing an extended timestamp, arrival times and particle counts.

Finally a coincidence is saved with links to the station events (for stations that triggered) and the input shower paramters. A coincidence is also saved if there are no events, which can be used to determine thetrigger efficiency.


\section{Realistic simulation}


In order to perform realistic simulations the cosmic-ray energy spectrum must be considered.

[TODO: Simulation with fluxes and azimuth/zenith distributions.

[TODO: Acceptance; angle, energy and particles.]

See detector response, cluster response and calibration.


\section{Analysis}

The direction reconstruction algorithm reported in [fokkema2012] can be improved for better accuracy. Several improvements can be made; first using more than 3 detection points, second account for the different altitudes of the detectors, and finally account for the shape of the shower front.


\subsection{Reconstruction in cartesian coordinates}

[]%[bits from montanus]


\subsection{Utilizing more detection points}

[]%[bits from montanus]


\subsection{Accounting for detector altitudes}

[]%[bits from montanus]


\subsection{Air shower front shape as seen by stations}

The shape of the shower front as seen by the detectors must be determined. Using the simulated dataset these characteristics of the air showers can be extracted. Even with the same starting parameters the shower front at ground level varies for each shower. Moreover, if the detection station position is shifted by a meter it will detect entirely different particles. The variations from shower to shower with the same starting parameters (particle and energy) and within the same shower can be estimated. These variations limit the accuracy of the shower reconstruction.

For showers with a proton primary from zenith \SI{0}{\degree} a selection of \num{40} showers [check] was made for the following energies \SIlist{e15;e15.5;e16;e16.5;e17}. These showers are used to investigate the variations between showers. Each of these showers was used in a detector simulation and simulated on a single station at several specific core distances. For each core distance each shower was used \num{5000} times with randomized azimuthal angles, by randomizing the location of the station along the chosen core distance circle. For each shower the median arrival time at each core distance was determined. For each shower energy these values were combined to get an average shower profile.

In figure \cref{fig:shower_detected_time_profile} the actual temporal lepton distribution and the detected (first arrival time) distributions are compared. For higher particle densities the detected shower profile is pushed more forward, since the probability that the first particle is closer to the front of the shower front is higher. At core distances where the detection/trigger probability drops the uncertainty in the detected profile increases, more simulations would be required to obtain an accurate profile, however, since the detection probability is so low the importance is negligible.

\begin{figure}
    \centering
    \includegraphics{plots/reconstructions/proton_E_16-0_Z_0-0_I_22_149042664_130233131}
    \caption{Compared raw shower front profile (open, gray) to detected shower profile (closed, blue). As a function of core distance the temporal distribution of leptons in the shower front is shown. The markers indicate the median arrival times, the colored bands include \SI{50}{\percent} of the particles. For the detected shower profile the arrival time of the first particle in a detector counts, if the station triggered.}
    \label{fig:shower_detected_time_profile}
\end{figure}

After combining the detected profiles from multiple showers a relation can be derrived between the time delay and core distance. For each shower energy the shower profile is fit using $t = a r ^ b$. In table [make table with fitted params] the fitted parameters are shown. This time delay is used in direction reconstruction.


\subsection{Reconstruction steps}

Raw data is first processed as described in \cref{ch:data_processing}. Because the core position is not yet known a simple direction reconstruction is performed which assumes a flat shower front. After this initial direction is determined the core position is estimated.

The shower direction can be used to improve the estimated particle density in a station by correcting for the longer path of the shower particles in the detectors, which are a factor $\cos^{-1} \theta$ higher than they would be if going vertically through the detectors. Simultaneously the effective surface of the detector is reduced by a factor $\cos \theta$. Both these effects need to be corrected for. The uncertainty in the detected density is at low particle densities dominated by Poisson statistics. The value to be considered is not the particle density but the number of detected particles. Therefore the number should be corrected for the longer path, but not the different effective area.

Using the estimated core position the direction reconstruction can use the core position to determine the radial core distance of each detector to the shower core. The reconstruction can then use non-flat shower fronts to reconstruct the core. The shape of a shower front
